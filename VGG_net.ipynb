{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73pTO_Tl1Plm"
      },
      "source": [
        "# VGG-Net\n",
        "\n",
        "#### Introduction\n",
        "\n",
        ">The full name of VGG is the Visual Geometry Group, which belongs to the Department of Science and Engineering of Oxford University. It has released a series of convolutional network models beginning with VGG, which can be applied to face recognition and image classification, from VGG16 to VGG19. The original purpose of VGG's research on the depth of convolutional networks is to understand how the depth of convolutional networks affects the accuracy and accuracy of large-scale image classification and recognition. -Deep-16 CNN), in order to deepen the number of network layers and to avoid too many parameters, a small 3x3 convolution kernel is used in all layers.\n",
        "\n",
        "<a href=\"http://ethereon.github.io/netscope/#/gist/dc5003de6943ea5a6b8b\" target=\"_blank\">Network Structure of VGG19</a>\n",
        "\n",
        "#### The network structure\n",
        "\n",
        ">The input of VGG is set to an RGB image of 224x244 size. The average RGB value is calculated for all images on the training set image, and then the image is input as an input to the VGG convolution network. A 3x3 or 1x1 filter is used, and the convolution step is fixed. . There are 3 VGG fully connected layers, which can vary from VGG11 to VGG19 according to the total number of convolutional layers + fully connected layers. The minimum VGG11 has 8 convolutional layers and 3 fully connected layers. The maximum VGG19 has 16 convolutional layers. +3 fully connected layers. In addition, the VGG network is not followed by a pooling layer behind each convolutional layer, or a total of 5 pooling layers distributed under different convolutional layers. The following figure is VGG Structure diagram:\n",
        "\n",
        "\n",
        "![title](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/vgg.png)\n",
        "\n",
        "![config](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/network.png)\n",
        "\n",
        "\n",
        ">VGG16 contains 16 layers and VGG19 contains 19 layers. A series of VGGs are exactly the same in the last three fully connected layers. The overall structure includes 5 sets of convolutional layers, followed by a MaxPool. The difference is that more and more cascaded convolutional layers are included in the five sets of convolutional layers .\n",
        "\n",
        "![config](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/config2.jpg)\n",
        "\n",
        "\n",
        "> Each convolutional layer in AlexNet contains only one convolution, and the size of the convolution kernel is 7 * 7 ,. In VGGNet, each convolution layer contains 2 to 4 convolution operations. The size of the convolution kernel is 3 * 3, the convolution step size is 1, the pooling kernel is 2 * 2, and the step size is 2. The most obvious improvement of VGGNet is to reduce the size of the convolution kernel and increase the number of convolution layers.\n",
        "\n",
        "\n",
        "![config](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/config3.jpg)\n",
        "\n",
        "\n",
        "> Using multiple convolution layers with smaller convolution kernels instead of a larger convolution layer with convolution kernels can reduce parameters on the one hand, and the author believes that it is equivalent to more non-linear mapping, which increases the Fit expression ability.\n",
        "\n",
        "![config](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/config4.png)\n",
        "\n",
        ">Two consecutive 3 * 3 convolutions are equivalent to a 5 * 5 receptive field, and three are equivalent to 7 * 7. The advantages of using three 3 * 3 convolutions instead of one 7 * 7 convolution are twofold : one, including three ReLu layers instead of one , makes the decision function more discriminative; and two, reducing parameters . For example, the input and output are all C channels. 3 convolutional layers using 3 * 3 require 3 (3 * 3 * C * C) = 27 * C * C, and 1 convolutional layer using 7 * 7 requires 7 * 7 * C * C = 49C * C. This can be seen as applying a kind of regularization to the 7 * 7 convolution, so that it is decomposed into three 3 * 3 convolutions.\n",
        "\n",
        ">The 1 * 1 convolution layer is mainly to increase the non-linearity of the decision function without affecting the receptive field of the convolution layer. Although the 1 * 1 convolution operation is linear, ReLu adds non-linearity.\n",
        "\n",
        "\n",
        "#### Network Configuration\n",
        "\n",
        "> Table 1 shows all network configurations. These networks follow the same design principles, but differ in depth.\n",
        "\n",
        "![config](https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/vgg/netconvgg.png)\n",
        "\n",
        ">This picture is definitely used when introducing VGG16. This picture contains a lot of information. My interpretation here may be limited. If you have any supplements, please leave a message.\n",
        "\n",
        "* **Number 1** : This is a comparison chart of 6 networks. From A to E, the network is getting deeper. Several layers have been added to verify the effect.\n",
        "\n",
        "* **Number 2** : Each column explains the structure of each network in detail.\n",
        "\n",
        "* **Number 3**: This is a correct way to do experiments, that is, use the simplest method to solve the problem , and then gradually optimize for the problems that occur.\n",
        "\n",
        "**Network A**: First mention a shallow network, this network can easily converge on ImageNet.\n",
        "And then?\n",
        "\n",
        "**Network A-LRN**: Add something that someone else (AlexNet) has experimented to say is effective (LRN), but it seems useless.\n",
        "And then?\n",
        "\n",
        "**Network B**: Then try adding 2 layers? Seems to be effective.\n",
        "And then?\n",
        "\n",
        "**Network C**: Add two more layers of 1 * 1 convolution, and it will definitely converge. The effect seems to be better. A little excited.\n",
        "And then?\n",
        "\n",
        "**Network D**: Change the 1 * 1 convolution kernel to 3 * 3. Try it. The effect has improved again. Seems to be the best (2014).\n",
        "\n",
        "#### Training\n",
        "\n",
        "**The optimization method** is a stochastic gradient descent SGD + momentum (0.9) with momentum.\n",
        "The batch size is 256.\n",
        "\n",
        "**Regularization** : L2 regularization is used, and the weight decay is 5e-4. Dropout is after the first two fully connected layers, p = 0.5.\n",
        "\n",
        "Although it is deeper and has more parameters than the AlexNet network, we speculate that VGGNet can converge in less cycles for two reasons: one, the greater depth and smaller convolutions bring implicit regularization ; Second, some layers of pre-training.\n",
        "\n",
        "**Parameter initialization** : For a shallow A network, parameters are randomly initialized, the weight w is sampled from N (0, 0.01), and the bias is initialized to 0. Then, for deeper networks, first the first four convolutional layers and three fully connected layers are initialized with the parameters of the A network. However, it was later discovered that it is also possible to directly initialize it without using pre-trained parameters.\n",
        "\n",
        "In order to obtain a 224 * 224 input image, each rescaled image is randomly cropped in each SGD iteration. In order to enhance the data set, the cropped image is also randomly flipped horizontally and RGB color shifted.\n",
        "\n",
        "#### Summary of VGGNet improvement points\n",
        "\n",
        "1. A smaller 3 * 3 convolution kernel and a deeper network are used . The stack of two 3 * 3 convolution kernels is relative to the field of view of a 5 * 5 convolution kernel, and the stack of three 3 * 3 convolution kernels is equivalent to the field of view of a 7 * 7 convolution kernel. In this way, there can be fewer parameters (3 stacked 3 * 3 structures have only 7 * 7 structural parameters (3 * 3 * 3) / (7 * 7) = 55%); on the other hand, they have more The non-linear transformation increases the ability of CNN to learn features.\n",
        "\n",
        "\n",
        "2. In the convolutional structure of VGGNet, a 1 * 1 convolution kernel is introduced. Without affecting the input and output dimensions, non-linear transformation is introduced to increase the expressive power of the network and reduce the amount of calculation.\n",
        "\n",
        "\n",
        "3. During training, first train a simple (low-level) VGGNet A-level network, and then use the weights of the A network to initialize the complex models that follow to speed up the convergence of training .\n",
        "\n",
        "\n",
        "#### Some basic questions\n",
        "\n",
        "**Q1: Why can 3 3x3 convolutions replace 7x7 convolutions?**\n",
        "\n",
        "***Answer 1***\n",
        "\n",
        "3 3x3 convolutions, using 3 non-linear activation functions, increasing non-linear expression capabilities, making the segmentation plane more separable\n",
        "Reduce the number of parameters. For the convolution kernel of C channels, 7x7 contains parameters , and the number of 3 3x3 parameters is greatly reduced.\n",
        "\n",
        "\n",
        "**Q2: The role of 1x1 convolution kernel**\n",
        "\n",
        "***Answer 2***\n",
        "\n",
        "Increase the nonlinearity of the model without affecting the receptive field\n",
        "1x1 winding machine is equivalent to linear transformation, and the non-linear activation function plays a non-linear role\n",
        "\n",
        "\n",
        "**Q3: The effect of network depth on results (in the same year, Google also independently released the network GoogleNet with a depth of 22 layers)**\n",
        "\n",
        "***Answer 3***\n",
        "\n",
        "VGG and GoogleNet models are deep\n",
        "Small convolution\n",
        "VGG only uses 3x3, while GoogleNet uses 1x1, 3x3, 5x5, the model is more complicated (the model began to use a large convolution kernel to reduce the calculation of the subsequent machine layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en62A2DR2LiM"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MRQK-PF2eBM"
      },
      "source": [
        "## From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MXBLwHBz0dpp"
      },
      "outputs": [],
      "source": [
        "# Importing the required library\n",
        "\n",
        "from tensorflow import keras\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9BblExrKa5f",
        "outputId": "33c2bdb2-4e85-402c-9a50-6212d93a77eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 50000\n",
            "Number of testing samples: 10000\n"
          ]
        }
      ],
      "source": [
        "# Get Data\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(f\"Number of training samples: {len(x_train)}\")\n",
        "print(f\"Number of testing samples: {len(x_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42ov7_htKa3O",
        "outputId": "19aa1e33-22e2-4454-8e99-6595477fd0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R2GV1IeKax7",
        "outputId": "cdee2d3d-711b-4fb0-e678-415eca0b59fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_13 (Conv2D)          (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 8, 8, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 4, 4, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 2, 2, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 1, 1, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33666897 (128.43 MB)\n",
            "Trainable params: 33666897 (128.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(32,32,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=17, activation=\"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0oz6ZjnzKavL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9mSusDZLLli",
        "outputId": "b854f98c-2be5-4a1f-d29e-c253b28d5d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:50: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Error when checking input: expected conv2d_input to have shape (224, 224, 3) but got array with shape (32, 32, 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:856\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    855\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_arrays_v1.py:698\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    693\u001b[0m ):\n\u001b[0;32m    694\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(\n\u001b[0;32m    695\u001b[0m         batch_size, steps_per_epoch, x\n\u001b[0;32m    696\u001b[0m     )\n\u001b[1;32m--> 698\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[0;32m    712\u001b[0m         val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    713\u001b[0m             validation_data, batch_size, validation_steps\n\u001b[0;32m    714\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2652\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2644\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2648\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(_is_symbolic_tensor(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_inputs)\n\u001b[0;32m   2649\u001b[0m ):\n\u001b[0;32m   2650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2693\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2690\u001b[0m \u001b[38;5;66;03m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)):\n\u001b[0;32m   2692\u001b[0m     \u001b[38;5;66;03m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2693\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_input_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeed_input_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;66;03m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;66;03m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;66;03m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;66;03m# standardization code.\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n",
            "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_utils_v1.py:731\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m dim, ref_dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_shape, shape):\n\u001b[0;32m    726\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    727\u001b[0m                     ref_dim \u001b[38;5;241m!=\u001b[39m dim\n\u001b[0;32m    728\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m ref_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m                 ):\n\u001b[1;32m--> 731\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when checking \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m                         \u001b[38;5;241m+\u001b[39m exception_prefix\n\u001b[0;32m    734\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m                         \u001b[38;5;241m+\u001b[39m names[i]\n\u001b[0;32m    736\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to have shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(shape)\n\u001b[0;32m    738\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got array with shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data_shape)\n\u001b[0;32m    740\u001b[0m                     )\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have shape (224, 224, 3) but got array with shape (32, 32, 3)"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,validation_split=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1sZLOuALLjj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHmxY2i73UbV"
      },
      "source": [
        "## VGG Pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "xzYamMxKUISY",
        "outputId": "2a894c08-f371-4c71-fe23-ad7262dc637d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?/export=download&id=12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP\n",
            "To: /content/catdog.zip\n",
            "100%|██████████| 9.09M/9.09M [00:00<00:00, 118MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'catdog.zip'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the data from g drive\n",
        "\n",
        "import gdown\n",
        "url = \"https://drive.google.com/file/d/12jiQxJzYSYl3wnC8x5wHAhRzzJmmsCXP/view?usp=sharing\"\n",
        "file_id = url.split(\"/\")[-2]\n",
        "print(file_id)\n",
        "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
        "gdown.download(prefix+file_id, \"catdog.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X87fIgssUIKZ",
        "outputId": "3c19503d-4f08-4f2a-8965-3297a7044b50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  catdog.zip\n",
            "   creating: train/\n",
            "   creating: train/Cat/\n",
            "  inflating: train/Cat/0.jpg         \n",
            "  inflating: train/Cat/1.jpg         \n",
            "  inflating: train/Cat/2.jpg         \n",
            "  inflating: train/Cat/cat.2405.jpg  \n",
            "  inflating: train/Cat/cat.2406.jpg  \n",
            "  inflating: train/Cat/cat.2436.jpg  \n",
            "  inflating: train/Cat/cat.2437.jpg  \n",
            "  inflating: train/Cat/cat.2438.jpg  \n",
            "  inflating: train/Cat/cat.2439.jpg  \n",
            "  inflating: train/Cat/cat.2440.jpg  \n",
            "  inflating: train/Cat/cat.2441.jpg  \n",
            "  inflating: train/Cat/cat.2442.jpg  \n",
            "  inflating: train/Cat/cat.2443.jpg  \n",
            "  inflating: train/Cat/cat.2444.jpg  \n",
            "  inflating: train/Cat/cat.2445.jpg  \n",
            "  inflating: train/Cat/cat.2446.jpg  \n",
            "  inflating: train/Cat/cat.2447.jpg  \n",
            "  inflating: train/Cat/cat.2448.jpg  \n",
            "  inflating: train/Cat/cat.2449.jpg  \n",
            "  inflating: train/Cat/cat.2450.jpg  \n",
            "  inflating: train/Cat/cat.2451.jpg  \n",
            "  inflating: train/Cat/cat.2452.jpg  \n",
            "  inflating: train/Cat/cat.2453.jpg  \n",
            "  inflating: train/Cat/cat.2454.jpg  \n",
            "  inflating: train/Cat/cat.2455.jpg  \n",
            "  inflating: train/Cat/cat.2456.jpg  \n",
            "  inflating: train/Cat/cat.2457.jpg  \n",
            "  inflating: train/Cat/cat.2458.jpg  \n",
            "  inflating: train/Cat/cat.2459.jpg  \n",
            "  inflating: train/Cat/cat.2460.jpg  \n",
            "  inflating: train/Cat/cat.2461.jpg  \n",
            "  inflating: train/Cat/cat.2462.jpg  \n",
            "  inflating: train/Cat/cat.2463.jpg  \n",
            "  inflating: train/Cat/cat.2464.jpg  \n",
            "  inflating: train/Cat/cat.855.jpg   \n",
            "  inflating: train/Cat/cat.856.jpg   \n",
            "  inflating: train/Cat/cat.857.jpg   \n",
            "  inflating: train/Cat/cat.858.jpg   \n",
            "  inflating: train/Cat/cat.859.jpg   \n",
            "  inflating: train/Cat/cat.86.jpg    \n",
            "  inflating: train/Cat/cat.860.jpg   \n",
            "  inflating: train/Cat/cat.861.jpg   \n",
            "  inflating: train/Cat/cat.862.jpg   \n",
            "  inflating: train/Cat/cat.863.jpg   \n",
            "  inflating: train/Cat/cat.864.jpg   \n",
            "  inflating: train/Cat/cat.865.jpg   \n",
            "  inflating: train/Cat/cat.866.jpg   \n",
            "  inflating: train/Cat/cat.867.jpg   \n",
            "  inflating: train/Cat/cat.868.jpg   \n",
            "  inflating: train/Cat/cat.869.jpg   \n",
            "  inflating: train/Cat/cat.87.jpg    \n",
            "  inflating: train/Cat/cat.870.jpg   \n",
            "  inflating: train/Cat/cat.871.jpg   \n",
            "  inflating: train/Cat/cat.872.jpg   \n",
            "  inflating: train/Cat/cat.873.jpg   \n",
            "  inflating: train/Cat/cat.874.jpg   \n",
            "  inflating: train/Cat/cat.875.jpg   \n",
            "  inflating: train/Cat/cat.876.jpg   \n",
            "  inflating: train/Cat/cat.877.jpg   \n",
            "  inflating: train/Cat/cat.878.jpg   \n",
            "  inflating: train/Cat/cat.879.jpg   \n",
            "  inflating: train/Cat/cat.88.jpg    \n",
            "  inflating: train/Cat/cat.880.jpg   \n",
            "  inflating: train/Cat/cat.881.jpg   \n",
            "  inflating: train/Cat/cat.882.jpg   \n",
            "  inflating: train/Cat/cat.883.jpg   \n",
            "  inflating: train/Cat/cat.884.jpg   \n",
            "  inflating: train/Cat/cat.885.jpg   \n",
            "  inflating: train/Cat/cat.886.jpg   \n",
            "  inflating: train/Cat/cat.887.jpg   \n",
            "  inflating: train/Cat/cat.888.jpg   \n",
            "  inflating: train/Cat/cat.889.jpg   \n",
            "  inflating: train/Cat/cat.89.jpg    \n",
            "  inflating: train/Cat/cat.890.jpg   \n",
            "  inflating: train/Cat/cat.891.jpg   \n",
            "  inflating: train/Cat/cat.892.jpg   \n",
            "  inflating: train/Cat/cat.893.jpg   \n",
            "  inflating: train/Cat/cat.894.jpg   \n",
            "  inflating: train/Cat/cat.895.jpg   \n",
            "  inflating: train/Cat/cat.896.jpg   \n",
            "  inflating: train/Cat/cat.897.jpg   \n",
            "  inflating: train/Cat/cat.898.jpg   \n",
            "  inflating: train/Cat/cat.899.jpg   \n",
            "  inflating: train/Cat/cat.9.jpg     \n",
            "  inflating: train/Cat/cat.90.jpg    \n",
            "  inflating: train/Cat/cat.900.jpg   \n",
            "  inflating: train/Cat/cat.901.jpg   \n",
            "  inflating: train/Cat/cat.902.jpg   \n",
            "  inflating: train/Cat/cat.903.jpg   \n",
            "  inflating: train/Cat/cat.904.jpg   \n",
            "  inflating: train/Cat/cat.905.jpg   \n",
            "  inflating: train/Cat/cat.906.jpg   \n",
            "  inflating: train/Cat/cat.907.jpg   \n",
            "  inflating: train/Cat/cat.908.jpg   \n",
            "  inflating: train/Cat/cat.909.jpg   \n",
            "  inflating: train/Cat/cat.91.jpg    \n",
            "  inflating: train/Cat/cat.910.jpg   \n",
            "  inflating: train/Cat/cat.911.jpg   \n",
            "  inflating: train/Cat/cat.912.jpg   \n",
            "  inflating: train/Cat/cat.913.jpg   \n",
            "  inflating: train/Cat/cat.914.jpg   \n",
            "  inflating: train/Cat/cat.915.jpg   \n",
            "  inflating: train/Cat/cat.916.jpg   \n",
            "  inflating: train/Cat/cat.917.jpg   \n",
            "  inflating: train/Cat/cat.918.jpg   \n",
            "  inflating: train/Cat/cat.919.jpg   \n",
            "  inflating: train/Cat/cat.92.jpg    \n",
            "  inflating: train/Cat/cat.920.jpg   \n",
            "  inflating: train/Cat/cat.93.jpg    \n",
            "  inflating: train/Cat/cat.94.jpg    \n",
            "  inflating: train/Cat/cat.946.jpg   \n",
            "  inflating: train/Cat/cat.947.jpg   \n",
            "  inflating: train/Cat/cat.948.jpg   \n",
            "  inflating: train/Cat/cat.949.jpg   \n",
            "  inflating: train/Cat/cat.95.jpg    \n",
            "  inflating: train/Cat/cat.950.jpg   \n",
            "  inflating: train/Cat/cat.951.jpg   \n",
            "  inflating: train/Cat/cat.952.jpg   \n",
            "  inflating: train/Cat/cat.953.jpg   \n",
            "  inflating: train/Cat/cat.954.jpg   \n",
            "  inflating: train/Cat/cat.955.jpg   \n",
            "  inflating: train/Cat/cat.956.jpg   \n",
            "  inflating: train/Cat/cat.957.jpg   \n",
            "  inflating: train/Cat/cat.958.jpg   \n",
            "  inflating: train/Cat/cat.959.jpg   \n",
            "  inflating: train/Cat/cat.96.jpg    \n",
            "  inflating: train/Cat/cat.960.jpg   \n",
            "  inflating: train/Cat/cat.961.jpg   \n",
            "  inflating: train/Cat/cat.962.jpg   \n",
            "  inflating: train/Cat/cat.963.jpg   \n",
            "  inflating: train/Cat/cat.964.jpg   \n",
            "  inflating: train/Cat/cat.965.jpg   \n",
            "  inflating: train/Cat/cat.966.jpg   \n",
            "  inflating: train/Cat/cat.967.jpg   \n",
            "  inflating: train/Cat/cat.968.jpg   \n",
            "  inflating: train/Cat/cat.969.jpg   \n",
            "  inflating: train/Cat/cat.97.jpg    \n",
            "  inflating: train/Cat/cat.970.jpg   \n",
            "  inflating: train/Cat/cat.971.jpg   \n",
            "  inflating: train/Cat/cat.972.jpg   \n",
            "  inflating: train/Cat/cat.973.jpg   \n",
            "  inflating: train/Cat/cat.974.jpg   \n",
            "  inflating: train/Cat/cat.975.jpg   \n",
            "  inflating: train/Cat/cat.976.jpg   \n",
            "  inflating: train/Cat/cat.977.jpg   \n",
            "  inflating: train/Cat/cat.978.jpg   \n",
            "  inflating: train/Cat/cat.979.jpg   \n",
            "  inflating: train/Cat/cat.98.jpg    \n",
            "  inflating: train/Cat/cat.980.jpg   \n",
            "  inflating: train/Cat/cat.981.jpg   \n",
            "  inflating: train/Cat/cat.982.jpg   \n",
            "  inflating: train/Cat/cat.983.jpg   \n",
            "  inflating: train/Cat/cat.984.jpg   \n",
            "  inflating: train/Cat/cat.985.jpg   \n",
            "  inflating: train/Cat/cat.986.jpg   \n",
            "  inflating: train/Cat/cat.987.jpg   \n",
            "  inflating: train/Cat/cat.988.jpg   \n",
            "  inflating: train/Cat/cat.989.jpg   \n",
            "  inflating: train/Cat/cat.99.jpg    \n",
            "  inflating: train/Cat/cat.990.jpg   \n",
            "  inflating: train/Cat/cat.991.jpg   \n",
            "  inflating: train/Cat/cat.992.jpg   \n",
            "  inflating: train/Cat/cat.993.jpg   \n",
            "  inflating: train/Cat/cat.994.jpg   \n",
            "  inflating: train/Cat/cat.995.jpg   \n",
            "  inflating: train/Cat/cat.996.jpg   \n",
            "  inflating: train/Cat/cat.997.jpg   \n",
            "  inflating: train/Cat/cat.998.jpg   \n",
            "  inflating: train/Cat/cat.999.jpg   \n",
            "   creating: train/Dog/\n",
            "  inflating: train/Dog/10493.jpg     \n",
            "  inflating: train/Dog/11785.jpg     \n",
            "  inflating: train/Dog/9839.jpg      \n",
            "  inflating: train/Dog/dog.2432.jpg  \n",
            "  inflating: train/Dog/dog.2433.jpg  \n",
            "  inflating: train/Dog/dog.2434.jpg  \n",
            "  inflating: train/Dog/dog.2435.jpg  \n",
            "  inflating: train/Dog/dog.2436.jpg  \n",
            "  inflating: train/Dog/dog.2437.jpg  \n",
            "  inflating: train/Dog/dog.2438.jpg  \n",
            "  inflating: train/Dog/dog.2439.jpg  \n",
            "  inflating: train/Dog/dog.2440.jpg  \n",
            "  inflating: train/Dog/dog.2441.jpg  \n",
            "  inflating: train/Dog/dog.2442.jpg  \n",
            "  inflating: train/Dog/dog.2443.jpg  \n",
            "  inflating: train/Dog/dog.2444.jpg  \n",
            "  inflating: train/Dog/dog.2445.jpg  \n",
            "  inflating: train/Dog/dog.2446.jpg  \n",
            "  inflating: train/Dog/dog.2447.jpg  \n",
            "  inflating: train/Dog/dog.2448.jpg  \n",
            "  inflating: train/Dog/dog.2449.jpg  \n",
            "  inflating: train/Dog/dog.2450.jpg  \n",
            "  inflating: train/Dog/dog.2451.jpg  \n",
            "  inflating: train/Dog/dog.2452.jpg  \n",
            "  inflating: train/Dog/dog.2453.jpg  \n",
            "  inflating: train/Dog/dog.2454.jpg  \n",
            "  inflating: train/Dog/dog.2455.jpg  \n",
            "  inflating: train/Dog/dog.2456.jpg  \n",
            "  inflating: train/Dog/dog.2457.jpg  \n",
            "  inflating: train/Dog/dog.2458.jpg  \n",
            "  inflating: train/Dog/dog.2459.jpg  \n",
            "  inflating: train/Dog/dog.2460.jpg  \n",
            "  inflating: train/Dog/dog.2461.jpg  \n",
            "  inflating: train/Dog/dog.844.jpg   \n",
            "  inflating: train/Dog/dog.845.jpg   \n",
            "  inflating: train/Dog/dog.846.jpg   \n",
            "  inflating: train/Dog/dog.847.jpg   \n",
            "  inflating: train/Dog/dog.848.jpg   \n",
            "  inflating: train/Dog/dog.849.jpg   \n",
            "  inflating: train/Dog/dog.85.jpg    \n",
            "  inflating: train/Dog/dog.850.jpg   \n",
            "  inflating: train/Dog/dog.851.jpg   \n",
            "  inflating: train/Dog/dog.852.jpg   \n",
            "  inflating: train/Dog/dog.853.jpg   \n",
            "  inflating: train/Dog/dog.854.jpg   \n",
            "  inflating: train/Dog/dog.855.jpg   \n",
            "  inflating: train/Dog/dog.856.jpg   \n",
            "  inflating: train/Dog/dog.857.jpg   \n",
            "  inflating: train/Dog/dog.858.jpg   \n",
            "  inflating: train/Dog/dog.859.jpg   \n",
            "  inflating: train/Dog/dog.86.jpg    \n",
            "  inflating: train/Dog/dog.860.jpg   \n",
            "  inflating: train/Dog/dog.861.jpg   \n",
            "  inflating: train/Dog/dog.862.jpg   \n",
            "  inflating: train/Dog/dog.863.jpg   \n",
            "  inflating: train/Dog/dog.864.jpg   \n",
            "  inflating: train/Dog/dog.865.jpg   \n",
            "  inflating: train/Dog/dog.866.jpg   \n",
            "  inflating: train/Dog/dog.867.jpg   \n",
            "  inflating: train/Dog/dog.868.jpg   \n",
            "  inflating: train/Dog/dog.869.jpg   \n",
            "  inflating: train/Dog/dog.87.jpg    \n",
            "  inflating: train/Dog/dog.870.jpg   \n",
            "  inflating: train/Dog/dog.871.jpg   \n",
            "  inflating: train/Dog/dog.872.jpg   \n",
            "  inflating: train/Dog/dog.873.jpg   \n",
            "  inflating: train/Dog/dog.874.jpg   \n",
            "  inflating: train/Dog/dog.875.jpg   \n",
            "  inflating: train/Dog/dog.876.jpg   \n",
            "  inflating: train/Dog/dog.877.jpg   \n",
            "  inflating: train/Dog/dog.878.jpg   \n",
            "  inflating: train/Dog/dog.879.jpg   \n",
            "  inflating: train/Dog/dog.88.jpg    \n",
            "  inflating: train/Dog/dog.880.jpg   \n",
            "  inflating: train/Dog/dog.881.jpg   \n",
            "  inflating: train/Dog/dog.882.jpg   \n",
            "  inflating: train/Dog/dog.883.jpg   \n",
            "  inflating: train/Dog/dog.884.jpg   \n",
            "  inflating: train/Dog/dog.885.jpg   \n",
            "  inflating: train/Dog/dog.886.jpg   \n",
            "  inflating: train/Dog/dog.887.jpg   \n",
            "  inflating: train/Dog/dog.888.jpg   \n",
            "  inflating: train/Dog/dog.889.jpg   \n",
            "  inflating: train/Dog/dog.89.jpg    \n",
            "  inflating: train/Dog/dog.890.jpg   \n",
            "  inflating: train/Dog/dog.891.jpg   \n",
            "  inflating: train/Dog/dog.892.jpg   \n",
            "  inflating: train/Dog/dog.893.jpg   \n",
            "  inflating: train/Dog/dog.894.jpg   \n",
            "  inflating: train/Dog/dog.895.jpg   \n",
            "  inflating: train/Dog/dog.896.jpg   \n",
            "  inflating: train/Dog/dog.897.jpg   \n",
            "  inflating: train/Dog/dog.898.jpg   \n",
            "  inflating: train/Dog/dog.9.jpg     \n",
            "  inflating: train/Dog/dog.90.jpg    \n",
            "  inflating: train/Dog/dog.91.jpg    \n",
            "  inflating: train/Dog/dog.92.jpg    \n",
            "  inflating: train/Dog/dog.93.jpg    \n",
            "  inflating: train/Dog/dog.936.jpg   \n",
            "  inflating: train/Dog/dog.937.jpg   \n",
            "  inflating: train/Dog/dog.938.jpg   \n",
            "  inflating: train/Dog/dog.939.jpg   \n",
            "  inflating: train/Dog/dog.94.jpg    \n",
            "  inflating: train/Dog/dog.940.jpg   \n",
            "  inflating: train/Dog/dog.941.jpg   \n",
            "  inflating: train/Dog/dog.942.jpg   \n",
            "  inflating: train/Dog/dog.943.jpg   \n",
            "  inflating: train/Dog/dog.944.jpg   \n",
            "  inflating: train/Dog/dog.945.jpg   \n",
            "  inflating: train/Dog/dog.946.jpg   \n",
            "  inflating: train/Dog/dog.947.jpg   \n",
            "  inflating: train/Dog/dog.948.jpg   \n",
            "  inflating: train/Dog/dog.949.jpg   \n",
            "  inflating: train/Dog/dog.95.jpg    \n",
            "  inflating: train/Dog/dog.950.jpg   \n",
            "  inflating: train/Dog/dog.951.jpg   \n",
            "  inflating: train/Dog/dog.952.jpg   \n",
            "  inflating: train/Dog/dog.953.jpg   \n",
            "  inflating: train/Dog/dog.954.jpg   \n",
            "  inflating: train/Dog/dog.955.jpg   \n",
            "  inflating: train/Dog/dog.956.jpg   \n",
            "  inflating: train/Dog/dog.957.jpg   \n",
            "  inflating: train/Dog/dog.958.jpg   \n",
            "  inflating: train/Dog/dog.959.jpg   \n",
            "  inflating: train/Dog/dog.96.jpg    \n",
            "  inflating: train/Dog/dog.960.jpg   \n",
            "  inflating: train/Dog/dog.961.jpg   \n",
            "  inflating: train/Dog/dog.962.jpg   \n",
            "  inflating: train/Dog/dog.963.jpg   \n",
            "  inflating: train/Dog/dog.964.jpg   \n",
            "  inflating: train/Dog/dog.965.jpg   \n",
            "  inflating: train/Dog/dog.966.jpg   \n",
            "  inflating: train/Dog/dog.967.jpg   \n",
            "  inflating: train/Dog/dog.968.jpg   \n",
            "  inflating: train/Dog/dog.969.jpg   \n",
            "  inflating: train/Dog/dog.97.jpg    \n",
            "  inflating: train/Dog/dog.970.jpg   \n",
            "  inflating: train/Dog/dog.971.jpg   \n",
            "  inflating: train/Dog/dog.972.jpg   \n",
            "  inflating: train/Dog/dog.973.jpg   \n",
            "  inflating: train/Dog/dog.974.jpg   \n",
            "  inflating: train/Dog/dog.975.jpg   \n",
            "  inflating: train/Dog/dog.976.jpg   \n",
            "  inflating: train/Dog/dog.977.jpg   \n",
            "  inflating: train/Dog/dog.978.jpg   \n",
            "  inflating: train/Dog/dog.979.jpg   \n",
            "  inflating: train/Dog/dog.98.jpg    \n",
            "  inflating: train/Dog/dog.980.jpg   \n",
            "  inflating: train/Dog/dog.981.jpg   \n",
            "  inflating: train/Dog/dog.982.jpg   \n",
            "  inflating: train/Dog/dog.983.jpg   \n",
            "  inflating: train/Dog/dog.984.jpg   \n",
            "  inflating: train/Dog/dog.985.jpg   \n",
            "  inflating: train/Dog/dog.986.jpg   \n",
            "  inflating: train/Dog/dog.987.jpg   \n",
            "  inflating: train/Dog/dog.988.jpg   \n",
            "  inflating: train/Dog/dog.989.jpg   \n",
            "  inflating: train/Dog/dog.99.jpg    \n",
            "  inflating: train/Dog/dog.990.jpg   \n",
            "  inflating: train/Dog/dog.991.jpg   \n",
            "  inflating: train/Dog/dog.992.jpg   \n",
            "  inflating: train/Dog/dog.993.jpg   \n",
            "  inflating: train/Dog/dog.994.jpg   \n",
            "  inflating: train/Dog/dog.995.jpg   \n",
            "  inflating: train/Dog/dog.996.jpg   \n",
            "  inflating: train/Dog/dog.997.jpg   \n",
            "  inflating: train/Dog/dog.998.jpg   \n",
            "  inflating: train/Dog/dog.999.jpg   \n",
            "   creating: validation/\n",
            "   creating: validation/Cat/\n",
            "  inflating: validation/Cat/cat.2407.jpg  \n",
            "  inflating: validation/Cat/cat.2408.jpg  \n",
            "  inflating: validation/Cat/cat.2409.jpg  \n",
            "  inflating: validation/Cat/cat.2410.jpg  \n",
            "  inflating: validation/Cat/cat.2411.jpg  \n",
            "  inflating: validation/Cat/cat.2412.jpg  \n",
            "  inflating: validation/Cat/cat.2413.jpg  \n",
            "  inflating: validation/Cat/cat.2414.jpg  \n",
            "  inflating: validation/Cat/cat.2415.jpg  \n",
            "  inflating: validation/Cat/cat.2416.jpg  \n",
            "  inflating: validation/Cat/cat.2417.jpg  \n",
            "  inflating: validation/Cat/cat.2418.jpg  \n",
            "  inflating: validation/Cat/cat.2419.jpg  \n",
            "  inflating: validation/Cat/cat.2420.jpg  \n",
            "  inflating: validation/Cat/cat.2421.jpg  \n",
            "  inflating: validation/Cat/cat.2422.jpg  \n",
            "  inflating: validation/Cat/cat.2423.jpg  \n",
            "  inflating: validation/Cat/cat.2424.jpg  \n",
            "  inflating: validation/Cat/cat.2425.jpg  \n",
            "  inflating: validation/Cat/cat.2426.jpg  \n",
            "  inflating: validation/Cat/cat.2427.jpg  \n",
            "  inflating: validation/Cat/cat.2428.jpg  \n",
            "  inflating: validation/Cat/cat.2429.jpg  \n",
            "  inflating: validation/Cat/cat.2430.jpg  \n",
            "  inflating: validation/Cat/cat.2431.jpg  \n",
            "  inflating: validation/Cat/cat.2432.jpg  \n",
            "  inflating: validation/Cat/cat.2433.jpg  \n",
            "  inflating: validation/Cat/cat.2434.jpg  \n",
            "  inflating: validation/Cat/cat.2435.jpg  \n",
            "   creating: validation/Dog/\n",
            "  inflating: validation/Dog/dog.2402.jpg  \n",
            "  inflating: validation/Dog/dog.2403.jpg  \n",
            "  inflating: validation/Dog/dog.2404.jpg  \n",
            "  inflating: validation/Dog/dog.2405.jpg  \n",
            "  inflating: validation/Dog/dog.2406.jpg  \n",
            "  inflating: validation/Dog/dog.2407.jpg  \n",
            "  inflating: validation/Dog/dog.2408.jpg  \n",
            "  inflating: validation/Dog/dog.2409.jpg  \n",
            "  inflating: validation/Dog/dog.2410.jpg  \n",
            "  inflating: validation/Dog/dog.2411.jpg  \n",
            "  inflating: validation/Dog/dog.2412.jpg  \n",
            "  inflating: validation/Dog/dog.2413.jpg  \n",
            "  inflating: validation/Dog/dog.2414.jpg  \n",
            "  inflating: validation/Dog/dog.2415.jpg  \n",
            "  inflating: validation/Dog/dog.2416.jpg  \n",
            "  inflating: validation/Dog/dog.2417.jpg  \n",
            "  inflating: validation/Dog/dog.2418.jpg  \n",
            "  inflating: validation/Dog/dog.2419.jpg  \n",
            "  inflating: validation/Dog/dog.2420.jpg  \n",
            "  inflating: validation/Dog/dog.2421.jpg  \n",
            "  inflating: validation/Dog/dog.2422.jpg  \n",
            "  inflating: validation/Dog/dog.2423.jpg  \n",
            "  inflating: validation/Dog/dog.2424.jpg  \n",
            "  inflating: validation/Dog/dog.2425.jpg  \n",
            "  inflating: validation/Dog/dog.2426.jpg  \n",
            "  inflating: validation/Dog/dog.2427.jpg  \n",
            "  inflating: validation/Dog/dog.2428.jpg  \n",
            "  inflating: validation/Dog/dog.2429.jpg  \n",
            "  inflating: validation/Dog/dog.2430.jpg  \n",
            "  inflating: validation/Dog/dog.2431.jpg  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!unzip catdog.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2WiLq3PUIG2",
        "outputId": "ee01951b-967f-47a1-9abd-e9d81fe0e544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 337 images belonging to 2 classes.\n",
            "Found 59 images belonging to 2 classes.\n",
            "Epoch 1/5\n",
            "125/125 [==============================] - ETA: 0s - batch: 62.0000 - size: 15.2800 - loss: 2.4077 - acc: 0.9665"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 19s 138ms/step - batch: 62.0000 - size: 15.2800 - loss: 2.4086 - acc: 0.9665 - val_loss: 10.4672 - val_acc: 0.9297\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 17s 139ms/step - batch: 62.0000 - size: 15.2800 - loss: 0.1754 - acc: 0.9958 - val_loss: 3.5723 - val_acc: 0.9311\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 17s 135ms/step - batch: 62.0000 - size: 15.4000 - loss: 0.0877 - acc: 0.9984 - val_loss: 4.5018 - val_acc: 0.9473\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 17s 134ms/step - batch: 62.0000 - size: 15.2800 - loss: 0.0918 - acc: 0.9969 - val_loss: 3.2619 - val_acc: 0.9824\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 16s 129ms/step - batch: 62.0000 - size: 15.2800 - loss: 4.0183e-07 - acc: 1.0000 - val_loss: 2.7819 - val_acc: 0.9824\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "# Set the path to your training and validation data\n",
        "train_data_dir = '/content/train'\n",
        "validation_data_dir = '/content/validation'\n",
        "\n",
        "# Set the number of training and validation samples\n",
        "num_train_samples = 2000\n",
        "num_validation_samples = 800\n",
        "\n",
        "# Set the number of epochs and batch size\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "# Load the VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the base model as a layer\n",
        "model.add(base_model)\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Preprocess the training and validation data\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=num_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=num_validation_samples // batch_size)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('dog_cat_classifier.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE51Mp-QUIFX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoBS-sdaUICp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
